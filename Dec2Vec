# -*- coding: utf-8 -*-
"""
Created on Wed May 29 16:37:34 2019

@author: User
"""
#Doc2Vec
import pandas as pd
#import numpy as np
#import matplotlib.pyplot as plt 
#import nltk
import string
#from gensim.models.doc2vec import Doc2Vec, TaggedDocument
#from nltk.tokenize import word_tokenize
#from nltk.corpus import stopwords
#from nltk.stem.porter import PorterStemmer
#from os import listdir
from collections import Counter

housing  = pd.read_pickle("C:/Users/User/Desktop/Bachelorarbeit/Berlin_Daten/housing.pkl")

from collections import Counter
import string

#Sonderzeichen, Großbuchstaben, Zahlen werden erkannt und entfernt/als Kleinbuchstaben ausgegeben
#eine Vokabelliste wird erstellt
#nicht enthalten:
#Filter out German stop words
    #stop_words = set(stopwords.words('german'))
    #tokens = [w for w in tokens if not w in stop_words]
#Wörter auf den Wortstamm zurückführen
    #porter = PorterStemmer()
    #tokens = [porter.stem(word) for word in tokens]
vocab = Counter()
def cleandesc(filename): 
    for index, row in housing.iterrows(): #Dauer: 41 Minuten
        #einzelne Wörter herausfiltern
        tokens = row['Total Description'].split()
        #Sonderzeichen entfernen
        table = str.maketrans('', '', string.punctuation)
        tokens = [w.translate(table) for w in tokens]
        #Umwandlung Groß- in Kleinbuchstaben
        tokens = [word.lower() for word in tokens]
        #Zahlen herausfiltern
        tokens = [word for word in tokens if word.isalpha()]
        #bearbeitete Tokens wieder in die Spalte hinzufügen
        housing['Total Description'][index] = ' '.join(tokens)
        #Vokabelliste updaten
        vocab.update(tokens)
cleandesc(housing)

#nur Vokabeln, die oft genug vorkommen, werden weiter behalten
min_occurane = 5
tokens = [k for k,c in vocab.items() if c >= min_occurane]
#print(len(tokens)) 

# save list to file
def save_list(lines, filename):
	# convert lines to a single blob of text
	data = '\n'.join(lines)
	# open file
	file = open(filename, 'w', encoding= 'utf-8')
	# write text
	file.write(data)
	# close file
	file.close()
# save tokens to a vocabulary file
save_list(tokens, 'vocab.txt')
